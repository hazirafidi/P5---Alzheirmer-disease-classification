# -*- coding: utf-8 -*-
"""Img_classification_Alzheimer_disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ft_fRw9yx71Yu1WzYXG1zDy5GtCWl7FF

# Alzheimer's Disease Prediction

#### Prediction of Alzheimer disease based on MRI Preprocessed Dataset. The dataset can be found via [this link](https://www.kaggle.com/datasets/sachinkumar413/alzheimer-mri-dataset). The method used is image classification via transfer learning. The base model used for feature extraction is VGG16 and afterwards fine-tune at 10 layers ahead.
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip -uq "/content/drive/MyDrive/MRI Brain Scan.zip" -d "/content/sample_data"

# import necessary modules
import numpy as np
import pathlib
import matplotlib.pyplot as plt
import tensorflow as tf

# set variable for file_path and data_dir
file_path = r"/content/sample_data/Dataset"
data_dir = pathlib.Path(file_path)

# Split train and validation data
SEED = 1234
BATCH_SIZE = 10
IMG_SIZE = (160,160)

train_data = tf.keras.utils.image_dataset_from_directory(data_dir,
                                                         validation_split = 0.2,
                                                         subset = 'training',
                                                         seed = SEED,
                                                         image_size = IMG_SIZE,
                                                         batch_size = BATCH_SIZE,
                                                         shuffle = True
                                                         )

val_data = tf.keras.utils.image_dataset_from_directory(data_dir,
                                                       validation_split = 0.2,
                                                       subset = 'validation',
                                                       seed = SEED,
                                                       image_size = IMG_SIZE,
                                                       batch_size = BATCH_SIZE,
                                                       shuffle = True
                                                       )

print(f"total train data per batch: {len(train_data)}")
print(f"total validation data per batch: {len(val_data)}")

# set variable for class_names
class_names = train_data.class_names

# plot the train_data image
plt.figure(figsize=(10,10))
for images, labels in train_data.take(1):
  for i in range(10):
    ax = plt.subplot(5,5,i+1)
    plt.imshow(images[1].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis('off')

# perform split of validation data into test data
val_batches = tf.data.experimental.cardinality(val_data)
test_data = val_data.take(val_batches//2)
val_data = val_data.skip(val_batches//2)

print(f"Number of validation batches: {tf.data.experimental.cardinality(val_data)}")
print(f"Number of test batches: {tf.data.experimental.cardinality(test_data)}")

print(f"total train data: {len(train_data)}")
print(f"total validation data: {len(val_data)}")
print(f"total test data: {len(test_data)}")

# create prefetch dataset for better performance
AUTOTUNE = tf.data.AUTOTUNE

train_data = train_data.prefetch(buffer_size = AUTOTUNE)
val_data = val_data.prefetch(buffer_size = AUTOTUNE)
test_data = test_data.prefetch(buffer_size = AUTOTUNE)

# preprocess image input to rescale the image to fit into the pre-trained model
preprocess_input = tf.keras.applications.vgg16.preprocess_input

IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.VGG16(input_shape = IMG_SHAPE,
                                         include_top = False,
                                         weights = 'imagenet'
                                         )

base_model.summary()

# Freeze the entire base_model
base_model.trainable = False
base_model.summary()

# set variable for classfier layer
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

# set variable for output layer
nClass = len(class_names)
prediction_layer = tf.keras.layers.Dense(nClass, activation = 'softmax')

# re-construct the model
inputs = tf.keras.Input(shape = (160,160,3))
x = preprocess_input(inputs)
x = base_model(x, training = False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

# compile the model
base_learning_rate = 0.0001
model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = base_learning_rate),
              loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics = ['accuracy']
              )

# check the model
model.summary()

tf.keras.utils.plot_model(model)

loss0, accuracy0 = model.evaluate(val_data)

print("---------------------------------------------Before Training---------------------------------------------")
print("loss", loss0)
print("accuracy", accuracy0)

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard
from gc import callbacks
import datetime, os
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
es_callback = callback = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 2)

# start train the model
initial_epochs = 50 
history = model.fit(train_data,
                    epochs = initial_epochs,
                    validation_data = val_data,
                    callbacks = [tensorboard_callback, es_callback])

#tf.keras.backend.clear_session()

# Commented out IPython magic to ensure Python compatibility.
# View training accuracy and loss graph via tensorboard
# %tensorboard --logdir logs

# Perform FINE TUNING of the trained model
# In the first feature extraction training, the model was only trained a few layers on top of VGG19 base_model. The weights of the pretrained network were not updated during training
# Fine_tuning is method to increase performance even further or "fine-tunes" the weights of the top layer of the VGG19 model alongside the classifier added
# We need to unfreeze the top layers- pf the base model
base_model.trainable = True

# let's take a look at the model layers
print("Number of layers in the base model: ", len(base_model.layers))

# So, we are going to fine-tune 10 layers ahead
fine_tune_at = 10

# Freeze all the layer before "fine_tune_at" layer
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable = False

# compile the model again
# As you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage. Otherwise, your model could overfit very quickly.
model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer = tf.keras.optimizers.RMSprop(learning_rate = base_learning_rate/10),
              metrics = ['accuracy'])

# lets check the model again
model.summary()

# Now, we resume the model trianing from the last epochs
fine_tune_epochs = 50
total_epochs = initial_epochs + fine_tune_epochs

# train the model again
histroy_fine_tune = model.fit(train_data,
                              epochs = total_epochs,
                              initial_epoch = history.epoch[-1],
                              validation_data = val_data,
                              callbacks = [tensorboard_callback, es_callback])

# Commented out IPython magic to ensure Python compatibility.
# View training accuracy and loss graph via tensorboard
# %tensorboard --logdir logs

# Evaluate the model
print("-----------------------------------After Fine-tuning model..........................................")
model.evaluate(test_data)

# Use test data to make prediction
image_batch, label_batch = test_data.as_numpy_iterator().next()
predictions = model.predict_on_batch(image_batch)
class_predictions = np.argmax(predictions, axis=1)

# plot the prediction vs actual label
plt.figure(figsize = (20,20))
for i in range(10):
  axs = plt.subplot(5,5,i+1)
  plt.imshow(image_batch[i].astype("uint8"))
  current_prediction = class_names[class_predictions[i]]
  current_label = class_names[label_batch[i]]
  plt.title(f"Prediction: {current_prediction} \n Actual: {current_label}")
  plt.axis('off')